{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-17T00:09:06.756457Z",
     "end_time": "2023-06-17T00:09:07.809269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2260068\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 565018\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from datasets import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "import os\n",
    "from docx.shared import Pt\n",
    "# dataset = load_from_disk(\"data/cleaned-data-mc4-v2\")\n",
    "# dataset = load_from_disk(\"larger-data-model/modified-mc4\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# type(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-11T13:16:03.878928Z",
     "end_time": "2023-06-11T13:16:03.891927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['label', 'text'],\n    num_rows: 2\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_from_disk\n",
    "d = {'train':Dataset.from_dict({'label':['something'],'text':['something']}),\n",
    "     'test':Dataset.from_dict({'label':['something'],'text':['something']})\n",
    "     }\n",
    "t = {'train': Dataset.from_dict({'label':['something1'],'text':['something1']})}\n",
    "d = DatasetDict(d)\n",
    "# d['train']\n",
    "eval_dataset = concatenate_datasets([d[\"test\"], t['train']])\n",
    "# d['train'][0]['text']\n",
    "eval_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T23:56:10.327043Z",
     "end_time": "2023-06-16T23:56:10.355282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_pdf(file):\n",
    "    pdf = open(file, 'rb')\n",
    "    reader = PyPDF2.PdfReader(pdf)\n",
    "    dataset = []\n",
    "\n",
    "    for page in range(1, len(reader.pages)):\n",
    "        dataset.append({'text': reader.pages[page].extract_text()})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def read_docx(file):\n",
    "    document = Document(file)\n",
    "    text = []\n",
    "    for line in document.paragraphs:\n",
    "        text.append(line.text)\n",
    "\n",
    "    return [{'text': t} for t in text]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to convert the text to a single font\n",
    "def convert_to_single_font(text, font_name, font_size):\n",
    "    georgian_mapping = {\n",
    "        'a': 'ა', 'b': 'ბ', 'g': 'გ', 'd': 'დ', 'e': 'ე', 'v': 'ვ', 'z': 'ზ', 't': 'თ', 'i': 'ი', 'k': 'კ',\n",
    "        'l': 'ლ', 'm': 'მ', 'n': 'ნ', 'o': 'ო', 'p': 'პ', 'zh': 'ჟ', 'r': 'რ', 's': 'ს', 't': 'ტ', 'u': 'უ',\n",
    "        'f': 'ფ', 'q': 'ქ', 'gh': 'ღ', 'y': 'ყ', 'sh': 'შ', 'ch': 'ჩ', 'ts': 'ც', 'dz': 'ძ', 'tz': 'წ', 'tch': 'ჭ',\n",
    "        'kh': 'ხ', 'j': 'ჯ', 'h': 'ჰ', ' ': ' '\n",
    "    }\n",
    "\n",
    "    doc = Document()\n",
    "    paragraph = doc.add_paragraph()\n",
    "\n",
    "    run = paragraph.add_run()\n",
    "    font = run.font\n",
    "    font.name = font_name\n",
    "    font.size = Pt(font_size)\n",
    "\n",
    "    converted_text = ''\n",
    "    for char in text:\n",
    "        for c in char:\n",
    "            lowercase_char = char.lower()\n",
    "            georgian_char = georgian_mapping.get(lowercase_char, lowercase_char)\n",
    "            converted_text += georgian_char\n",
    "\n",
    "    return converted_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T23:56:15.145006Z",
     "end_time": "2023-06-16T23:56:15.160516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# assign directory\n",
    "directory = ['data/book-collection-elibrary/', 'data/books/']\n",
    "\n",
    "def read_books(directory_list):\n",
    "    book_datasets = []\n",
    "    for directory in directory_list:\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.is_file():\n",
    "                if entry.name.endswith('.pdf'):\n",
    "                    book_datasets.extend(read_pdf(entry.path))\n",
    "                elif entry.name.endswith('.docx'):\n",
    "                    book_datasets.extend(read_docx(entry.path))\n",
    "            elif entry.is_dir():\n",
    "                book_datasets.extend(read_books([entry.path]))\n",
    "\n",
    "    return Dataset.from_dict({'text': [d['text'] for d in book_datasets]})\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T23:56:21.149225Z",
     "end_time": "2023-06-16T23:56:21.160735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_dict = read_books(directory)\n",
    "new_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T12:57:21.535876Z",
     "end_time": "2023-06-15T13:24:49.302705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 527256\n})"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-11T14:46:37.247129Z",
     "end_time": "2023-06-11T14:46:37.250129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 2825086\n})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "new_dataset = concatenate_datasets([dataset['train'], new_dict, dataset['test'], dataset['validation']])\n",
    "new_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:45:04.120822Z",
     "end_time": "2023-06-15T13:45:04.172337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2260068\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 565018\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigger_dataset = new_dataset.train_test_split(test_size=0.2)\n",
    "bigger_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-15T13:45:38.453834Z",
     "end_time": "2023-06-15T13:45:38.887882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import TFAutoModelForMaskedLM, DistilBertTokenizerFast\n",
    "import re\n",
    "\n",
    "# greek = r\"[\\u0370-\\u03FF]+\"\n",
    "# chinese = r\"[\\u4E00-\\u9FFF。]+\"\n",
    "# italic = r\"[\\U0001D400-\\U0001D7FF]+\"\n",
    "# hindi = r\"[\\u0900-\\u097F]+\"\n",
    "# telugu = r\"[\\u0C00-\\u0C7F]+\"\n",
    "# kannada = r\"[\\u0C80-\\u0CFF]+\"\n",
    "# odia = r\"[\\u0B00-\\u0B7F]+\"\n",
    "# bengali = r\"[\\u0980-\\u09FF]+\"\n",
    "# geez = r\"[\\u1200-\\u137F]+\"\n",
    "# malayalam = r\"[\\u0D00-\\u0D7F]+\"\n",
    "# russian = \"[а-яА-ЯёЁ]+\"\n",
    "# emojis = r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\"\n",
    "# hangul = r\"[\\uAC00-\\uD7AF\\u1100-\\u11FF\\u3130-\\u318F\\uA960-\\uA97F\\uAC00-\\uD7A3]+\"\n",
    "# arabic = r\"[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF]+\"\n",
    "# additional = r\"[\\u2190-\\u2BFF\\u3000-\\u303F\\u25A0-\\u27BF\\ufe00-\\ufe0f\\ufe30-\\ufe6f\\U0001F300-\\U0001F5FF]+\"\n",
    "# punctuation = r'[^\\w\\s]'\n",
    "# latin_1 = r\"[ª²³º¼½¾ßæðøþαεηιορυω]\"\n",
    "# latin_b = r\"[đħıĳĸŀłŉŋœŧſƀƃƅƈƌƍƒƕƙƚƛƞƣƥƨƪƫƭƴƶƹƺƻƾƿǀǁǂǃǆǉǝǥǳȝȡȣȥȴȵȶȷȼȿɀɂɇɉɋɍɏɐɑɒɓɔɕɖɗɘəɚɛɜɝɞɟɠɡɢɣɤɥɦɧɨɩɪɫɬɭɮɯɰɱɲɳɴɵɶɷɸɹɺɻɼɽɾɿʀʁʂʃʄʅʆʇʈʉʊʋʌʍʎʏʐʑʒʓʔʕʖʗʘʙʚʛʜʝʞʟʠʡʢʣʤʥʦʧʨʩʪʫʬʭʮʯʰʱʲʳʴʵʶʷʸʹʺʻʼʽʾʿˀˁˆˇˈˉˊˋˌˍˎˏːˑˠˡˢˣˤˬˮ]\"\n",
    "# cryillic = r\"[а-яА-ЯёЁђєѕіјљњћџѡѣѥѧѩѫѭѯѱѳѵѹѻѽѿҁҋҍҏґғҕҗҙқҝҟҡңҥҧҩҫҭүұҳҵҷҹһҽҿӄӆӈӊӌӎӏӕәӡөӷӻӽӿԁԃԅԇԉԋԍԏԑԓԕԗԙԛԝԟԡԣԥԧԩԫԭԯՙաբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆև]\"\n",
    "\n",
    "\n",
    "# combined_pat = r'|'.join((greek, italic, chinese, hindi, telugu, kannada, odia, bengali, geez, malayalam, emojis, russian, hangul, arabic, additional, punctuation, r\"\\n\"))\n",
    "\n",
    "def remove_non_georgian_english_numbers(text):\n",
    "    # Define a regular expression pattern to match non-Georgian, non-English, and non-number characters\n",
    "    pattern = r\"[^a-zA-Zა-ჰ0-9\\s]+\"\n",
    "\n",
    "    # Use the sub() function from the re module to replace non-matching characters with an empty string\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    # Reduce consecutive spaces to a single space\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_dataset(example):\n",
    "    return {'text': remove_non_georgian_english_numbers(example['text'])}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:39:52.471577Z",
     "end_time": "2023-06-17T12:39:52.614899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2260068 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad2ff6c92f9e466e9d38efad4ae3b8ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/565018 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fdfb8653fd5450d8ab7e41a6381078a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2260068\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 565018\n    })\n})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset = dataset.map(clean_dataset)\n",
    "cleaned_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T23:58:02.919926Z",
     "end_time": "2023-06-17T00:03:07.878889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/28 shards):   0%|          | 0/2260068 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66323812502e42979f12162b3619ff12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/7 shards):   0%|          | 0/565018 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "020e842899d04112b2646e7136c9a744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_dataset.save_to_disk('larger-data-model/modified-mc4-v2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T00:08:16.146379Z",
     "end_time": "2023-06-17T00:08:23.511440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_from_disk('larger-data-model/modified-mc4-v2')\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def get_training_corpus():\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(train_dataset), 1000):\n",
    "        samples = train_dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, 30500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T00:09:24.067538Z",
     "end_time": "2023-06-17T01:07:35.320439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30500, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"larger-data-model/distilbert-tokenizer\")\n",
    "tokenizer = new_tokenizer\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T01:14:57.074392Z",
     "end_time": "2023-06-17T01:14:57.124207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2260068 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8c49e7ab63140a794766547dd8116c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/565018 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb5c5475adfb42058f7ac800f15fa023"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/28 shards):   0%|          | 0/2260068 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ca23028a6af4f56bf4cad4e8d170d58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/7 shards):   0%|          | 0/565018 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "565c07fce84249dab94bfaaa8b043e1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 2260068\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 565018\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets.save_to_disk('larger-data-model/tokenized-dataset')\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T01:16:02.771652Z",
     "end_time": "2023-06-17T02:17:28.417006Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "def group_texts(examples):\n",
    "    # შევაწებოთ ტექსტები\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # გამოვთვალოთ გაერთიანებული მონაცემების სიგრძე\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # თუ სხვებზე პატარაა ბოლო ნაწილი მოდელში არ შევუშვათ\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # მითითებული სიგრძის მიხედვით დავანაწევროდ\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # შევქმნათ ახალი სვეტი labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T02:17:28.418510Z",
     "end_time": "2023-06-17T02:17:28.446159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2260068 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ffb6a1ec56a47a88aba08c7b3fc6655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/565018 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f26ff16093d94ba5a00a16897a823fb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/45 shards):   0%|          | 0/2059516 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da1aa8ddeb4423182535bb6c98f932e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/12 shards):   0%|          | 0/511608 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8deaeb69a553403da1c4a00b0558168c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 2059516\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 511608\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets.save_to_disk('larger-data-model/lm_dataset')\n",
    "lm_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T02:17:28.432144Z",
     "end_time": "2023-06-17T03:49:09.582744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('larger-data-model/distilbert-tokenizer')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "whole_word_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:39:55.909471Z",
     "end_time": "2023-06-17T12:39:55.922468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "lm_datasets = load_from_disk('larger-data-model/lm_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:40:20.102121Z",
     "end_time": "2023-06-17T12:40:28.803885Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Davit6174\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py:951: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"train\"],\n",
    "    collate_fn=whole_word_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"test\"],\n",
    "    collate_fn=whole_word_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:40:32.361687Z",
     "end_time": "2023-06-17T12:40:32.719183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4090, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:40:37.725548Z",
     "end_time": "2023-06-17T12:40:37.760560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28859/64359 [============>.................] - ETA: 3:40:10 - loss: 6.3037"
     ]
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T12:40:39.591843Z",
     "end_time": "2023-06-17T19:48:04.318419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model.save_pretrained('larger-data-model/distilbert-base-uncased-fine-tuned')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T16:48:56.616384Z",
     "end_time": "2023-06-17T19:48:04.652176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T16:49:03.523597Z",
     "end_time": "2023-06-17T20:18:36.611224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "f = open(\"perplexity-large.txt\", \"w\")\n",
    "f.write(f\"Perplexity of model_large: {math.exp(eval_loss):.2f}\")\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-17T16:50:07.635640Z",
     "end_time": "2023-06-17T20:18:36.626375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
